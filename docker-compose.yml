version: '3.8'

services:
  surrealdb:
    image: surrealdb/surrealdb:latest
    ports:
      - "8000:8000"
    command: 
      - start
      - --log=trace
      - --user=root
      - --pass=root
      - memory
    environment:
      - RUST_LOG=debug

  lcm-designer:
    build: .
    ports:
      - "1420:1420"  # Frontend development server
      - "3001:3001"  # Backend API server
    volumes:
      - .:/app
      - /app/node_modules
      - /app/target
    environment:
      - NODE_ENV=development
      - DATABASE_URL=ws://surrealdb:8000/rpc
    working_dir: /app
    command: npm run dev
    depends_on:
      - surrealdb

  lcm-designer-server:
    build: .
    ports:
      - "3001:3001"
    volumes:
      - ./server:/app/server
    working_dir: /app/server
    command: npm start
    depends_on:
      - surrealdb
      - lcm-designer

  # Archer AI Engine (Python AI Sidecar)
  ai-engine:
    build: ./archer-ai-engine
    container_name: archer-ai-engine
    ports:
      - "8000:8000"
    environment:
      - AI_SIDECAR_HOST=0.0.0.0
      - AI_SIDECAR_PORT=8000
      - SURREALDB_URL=ws://surrealdb:8000/rpc
      - SURREALDB_NS=archer
      - SURREALDB_DB=main
      - LLM_PROVIDER=ollama
      # Note: host.docker.internal works on Docker Desktop (Mac/Windows)
      # For Linux, use host network mode or container IP address
      - OLLAMA_HOST=http://host.docker.internal:11434
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
    depends_on:
      - surrealdb
      - redis
    restart: unless-stopped
    # Uncomment for Linux hosts to access Ollama on host:
    # network_mode: host

  # Redis for AI Engine job queue
  redis:
    image: redis:7-alpine
    container_name: archer-redis
    ports:
      - "6379:6379"
    restart: unless-stopped

volumes:
  surrealdb_data:
