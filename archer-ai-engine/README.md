# Archer AI Engine

The AI microservice for the Archer ITSM Platform, providing a unified LLM Gateway and AI agent orchestration.

## ğŸ¯ Overview

The Archer AI Engine is a **Python FastAPI microservice** that operates alongside the existing Rust backend. It provides:

- **Pluggable LLM Gateway** - Unified interface for OpenAI, Anthropic, and Ollama
- **Transparent AI Operations** - Full Chain of Thought logging
- **Data Sovereignty** - Support for air-gapped local LLM deployment
- **Production-Ready Architecture** - Type-safe, tested, and containerized

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (React + TypeScript)             â”‚
â”‚                         Port 1420                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                               â”‚
              â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RUST CORE           â”‚     â”‚    PYTHON AI SIDECAR    â”‚
â”‚     (Port 3001)         â”‚â—„â”€â”€â”€â–ºâ”‚    (Port 8000)          â”‚
â”‚     Existing Backend    â”‚     â”‚    THIS SERVICE         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚         SurrealDB           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    LLM Backend (Pluggable)  â”‚
              â”‚  Ollama / OpenAI / Anthropic â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- (Optional) Ollama installed locally for local LLM support
- (Optional) OpenAI or Anthropic API keys for cloud LLMs

### Installation

```bash
# Clone and navigate to directory
cd archer-ai-engine

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env

# Edit .env with your configuration
```

### Running Locally

```bash
# Start the service
uvicorn src.main:app --reload --port 8000

# Or use Python directly
python -m src.main
```

The service will be available at:
- **API**: http://localhost:8000
- **Docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Using Docker

```bash
# Build and run with docker-compose
docker-compose up -d

# View logs
docker-compose logs -f ai-engine

# Stop services
docker-compose down
```

## ğŸ“š API Endpoints

### Health Checks

- `GET /health` - Basic health check
- `GET /health/live` - Liveness probe
- `GET /health/ready` - Readiness probe (checks LLM availability)
- `GET /health/providers` - Detailed provider health status

### Chat Completions

- `POST /api/v1/chat/completions` - Chat completion (streaming or non-streaming)

**Request Body:**
```json
{
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "model": "llama3.2",
  "temperature": 0.7,
  "max_tokens": 2048,
  "stream": false
}
```

**Response:**
```json
{
  "id": "uuid",
  "content": "Hello! How can I help you?",
  "model": "llama3.2",
  "provider": "ollama",
  "finish_reason": "stop",
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 8,
    "total_tokens": 18
  },
  "created_at": "2025-12-08T02:00:00Z"
}
```

### Model Management

- `GET /api/v1/models` - List all available models
- `GET /api/v1/models/{provider}` - List models for specific provider

## âš™ï¸ Configuration

Configuration is managed via environment variables. See `.env.example` for all options.

### LLM Providers

#### Ollama (Local, Default)

```env
LLM_PROVIDER=ollama
OLLAMA_HOST=http://localhost:11434
LLM_DEFAULT_MODEL=llama3.2
```

**Prerequisites:**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model
ollama pull llama3.2
```

#### OpenAI (Cloud)

```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
OPENAI_DEFAULT_MODEL=gpt-4o-mini
```

#### Anthropic (Cloud)

```env
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022
```

## ğŸ§ª Testing

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Type checking
mypy src

# Linting
ruff check src
```

## ğŸ“¦ Project Structure

```
archer-ai-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                      # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py                    # Pydantic Settings
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ dependencies.py          # Dependency injection
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ health.py            # Health endpoints
â”‚   â”‚       â”œâ”€â”€ chat.py              # Chat endpoints
â”‚   â”‚       â””â”€â”€ models.py            # Model listing
â”‚   â”‚
â”‚   â”œâ”€â”€ llm_gateway/
â”‚   â”‚   â”œâ”€â”€ base.py                  # Abstract LLM interface
â”‚   â”‚   â”œâ”€â”€ types.py                 # Pydantic models
â”‚   â”‚   â”œâ”€â”€ router.py                # LLM Router/Factory
â”‚   â”‚   â”œâ”€â”€ openai_adapter.py        # OpenAI implementation
â”‚   â”‚   â”œâ”€â”€ anthropic_adapter.py     # Anthropic implementation
â”‚   â”‚   â””â”€â”€ ollama_adapter.py        # Ollama implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                      # Future: AI Agents
â”‚   â”‚   â”œâ”€â”€ base.py                  # Base Agent class
â”‚   â”‚   â””â”€â”€ orchestrator.py          # Agent orchestrator
â”‚   â”‚
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ logging.py               # Structured logging
â”‚       â””â”€â”€ exceptions.py            # Custom exceptions
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                  # Pytest fixtures
â”‚   â”œâ”€â”€ test_health.py               # Health endpoint tests
â”‚   â”œâ”€â”€ test_chat.py                 # Chat endpoint tests
â”‚   â””â”€â”€ test_llm_gateway/
â”‚       â”œâ”€â”€ test_router.py           # Router tests
â”‚       â”œâ”€â”€ test_openai.py           # OpenAI adapter tests
â”‚       â”œâ”€â”€ test_anthropic.py        # Anthropic adapter tests
â”‚       â””â”€â”€ test_ollama.py           # Ollama adapter tests
â”‚
â”œâ”€â”€ requirements.txt                 # Production dependencies
â”œâ”€â”€ requirements-dev.txt             # Development dependencies
â”œâ”€â”€ Dockerfile                       # Container image
â”œâ”€â”€ docker-compose.yml               # Local stack
â”œâ”€â”€ .env.example                     # Environment template
â””â”€â”€ README.md                        # This file
```

## ğŸ”® Future Roadmap

This is **Phase 1** of the AI Engine. Future phases will add:

### Phase 2: AI Agents (Months 3-4)
- **Librarian Agent** - RAG system for knowledge management
- **Ticket Assistant** - Intelligent ticket triage and suggestions
- **Context Manager** - Unified context across ITSM, CMDB, and monitoring

### Phase 3: Autonomous Operations (Months 5-6)
- **Operations Agent** - Autonomous actions with human-in-the-loop
- **Monitoring Analyst** - Predictive anomaly detection
- **Approval Workflows** - Risk assessment and red button approvals

## ğŸ¤ Integration with Archer

The AI Engine integrates with the existing Archer platform:

- **Frontend (Port 1420)** - React UI makes API calls to AI Engine
- **Rust Backend (Port 3001)** - Core ITSM/CMDB operations
- **SurrealDB** - Shared database for data and vector embeddings

### Example: Frontend Integration

```typescript
// frontend/src/utils/aiClient.ts
const AI_ENGINE_URL = 'http://localhost:8000';

export async function chatWithAI(message: string) {
  const response = await fetch(`${AI_ENGINE_URL}/api/v1/chat/completions`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      messages: [{ role: 'user', content: message }],
      stream: false,
    }),
  });
  return response.json();
}
```

## ğŸ“„ License

Part of the Archer ITSM Platform.

## ğŸ™‹ Support

For issues or questions, please refer to the main Archer repository.
